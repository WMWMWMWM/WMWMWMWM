{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EARgM_sloaL",
        "outputId": "61b44be1-fa75-42c0-bc97-b93a3d3256df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A . . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n",
            ". A . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n",
            ". . A . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n",
            ". . . A . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n",
            ". . . . A \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n",
            ". . . . . \n",
            ". . . . A \n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n",
            ". . . . . \n",
            ". . . . . \n",
            ". . . . A \n",
            ". . . . . \n",
            ". . . . G \n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 199\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Run the script\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[1], line 187\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m    186\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPOAgent(state_shape, n_actions)\n\u001b[0;32m--> 187\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m    190\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(reward_history)\n",
            "Cell \u001b[0;32mIn[1], line 158\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, episodes, render_interval)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m render_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    157\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()  \u001b[38;5;66;03m# Show the current state of the environment\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add a small delay to visualize the environment\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Necessary Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Define Custom Warehouse Environment in OpenAI gym\n",
        "class WarehouseEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(WarehouseEnv, self).__init__()\n",
        "        # Define size/shape (of grid)\n",
        "        self.shape = (5, 5)  # Easy - start here for POC\n",
        "        # self.shape = (10, 10) # Hard\n",
        "        # Define action space\n",
        "        self.action_space = gym.spaces.Discrete(4)  # Up, Down, Left, Right\n",
        "        # Define observation space - custom environment\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=1, shape=self.shape, dtype=np.float32\n",
        "        ) #a matrix with our defined shape (grid layout) and 3 possible values defined by our low/high range [0 = empty, 0.5 = goal, 1 = present]\n",
        "        # Define goal\n",
        "        self.goal = (4, 4) #here a tuple defining our destination\n",
        "        # Reset to initial state\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Set the state of the environment to zeros\n",
        "        self.state = np.zeros(self.shape)\n",
        "        # Define where our agent is and then place them, assigning observation value\n",
        "        self.agent_position = [0, 0]\n",
        "        self.state[self.agent_position[0], self.agent_position[1]] = 1\n",
        "        # Place our goal, assigning observation value\n",
        "        self.state[self.goal[0], self.goal[1]] = 0.5\n",
        "        # Flatten to be compatible for neural network input layer\n",
        "        return self.state.flatten()\n",
        "\n",
        "    # Define what our action space will look like\n",
        "    def step(self, action): #when taking step, have our environment and action\n",
        "        movements = {\n",
        "            0: (-1, 0),  # Up\n",
        "            1: (1, 0),  # Down\n",
        "            2: (0, -1),  # Left\n",
        "            3: (0, 1),  # Right\n",
        "        }\n",
        "        #Given our action what is our new position\n",
        "        new_pos = [\n",
        "            self.agent_position[0] + movements[action][0],\n",
        "            self.agent_position[1] + movements[action][1],\n",
        "        ]\n",
        "        #if the new position is valid, save it\n",
        "        if 0 <= new_pos[0] < self.shape[0] and 0 <= new_pos[1] < self.shape[1]:\n",
        "            self.agent_position = new_pos\n",
        "        #rebuilding the board\n",
        "        self.state = np.zeros(self.shape)\n",
        "        self.state[self.agent_position[0], self.agent_position[1]] = 1\n",
        "        self.state[self.goal[0], self.goal[1]] = 0.5\n",
        "\n",
        "        done = False #start as not done\n",
        "        reward = -1 #small penalty when take a step\n",
        "        # if agent finds its position at the goal\n",
        "        if self.agent_position == list(self.goal):\n",
        "            reward = 10 #give reward\n",
        "            done = True #change done to \"true\"\n",
        "        # if not then return state flattened and return our metadata\n",
        "        return self.state.flatten(), reward, done, {}\n",
        "\n",
        "    #How do we want to represent our environment\n",
        "    def render(self, mode=\"human\"):\n",
        "        for i in range(self.shape[0]):\n",
        "            row = \"\"\n",
        "            for j in range(self.shape[1]):\n",
        "                if (i, j) == self.goal:\n",
        "                    row += \"G \"  # Goal\n",
        "                elif [i, j] == self.agent_position:\n",
        "                    row += \"A \"  # Agent\n",
        "                else:\n",
        "                    row += \". \"  # Empty space\n",
        "            print(row)\n",
        "        print()  # Blank line at the end for readability\n",
        "\n",
        "\n",
        "# PPO Agent - houses the 2 models to form the policy and understand how making decisions\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_shape, n_actions):\n",
        "        self.state_shape = state_shape\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = 0.99\n",
        "        self.clip_ratio = 0.2 #consider very large gradients up to this threshold (upper limit)\n",
        "        self.learning_rate = 0.001\n",
        "\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "\n",
        "    # Actor Model = Policy Network = taking actions\n",
        "    def build_actor(self):\n",
        "        #define our architecture\n",
        "        inputs = layers.Input(shape=self.state_shape) #input is our state shape\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs) #128 neurons\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(self.n_actions, activation=\"softmax\")(x) #OUTPUT IS OUR ACTION\n",
        "        #define and compile, give it a loss function to be compiled with the Adam optimizer\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # Critic Model = Value Network = QUALITY OF THE STATE ACTION PAIR - VIA MSE (will be comparing actual with predicted)\n",
        "    def build_critic(self):\n",
        "        #define our architecture\n",
        "        inputs = layers.Input(shape=self.state_shape)\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(1, activation=\"linear\")(x) #OUTPUT is the quality of\n",
        "         #define and compile, give it a loss function to be compiled with the Adam optimizer\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "            loss=\"mean_squared_error\",\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # Advantage for Training the Agent\n",
        "    def compute_advantage(self, rewards, values):\n",
        "        #Define local variables\n",
        "        advantages = []\n",
        "        discounted_sum = 0\n",
        "        #Calculate the discounted sums of particular state given observations and rewards\n",
        "        for reward, value in zip(reversed(rewards), reversed(values)):\n",
        "            discounted_sum = reward + self.gamma * discounted_sum - value\n",
        "            advantages.insert(0, discounted_sum)\n",
        "        return np.array(advantages)\n",
        "\n",
        "    # Train our A+C Networks\n",
        "    def train(self, states, actions, rewards, values, old_probs):\n",
        "        #Set out variables\n",
        "        advantages = self.compute_advantage(rewards, values)\n",
        "        actions_one_hot = tf.keras.utils.to_categorical(actions, self.n_actions) #actions as 1-hot vector\n",
        "\n",
        "        # Update Actor via gradients of policy and weights in the actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            probs = self.actor(states)\n",
        "            new_probs = tf.reduce_sum(probs * actions_one_hot, axis=1)\n",
        "            ratio = new_probs / (old_probs + 1e-10)\n",
        "            #apply the clip advantage threshold\n",
        "            clip_advantage = (\n",
        "                tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
        "                * advantages\n",
        "            )\n",
        "            #our loss function\n",
        "            loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clip_advantage))\n",
        "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
        "        self.actor.optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
        "\n",
        "        # Update Critic\n",
        "        self.critic.train_on_batch(states, rewards)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "def train_agent(env, agent, episodes=500, render_interval=100):\n",
        "    #track of our training history\n",
        "    reward_history = []\n",
        "    #put our policy against many different episodes to collect info\n",
        "    for episode in range(episodes):\n",
        "        #Reset environment\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        #Initialize variables\n",
        "        states, actions, rewards, values, old_probs = [], [], [], [], []\n",
        "\n",
        "        while not done:\n",
        "            #Collect experience:\n",
        "            state = state.reshape(1, -1) #understand state\n",
        "            action_probs = agent.actor(state).numpy()[0] #decision based on state - actor\n",
        "            value = agent.critic(state).numpy()[0][0] #understand value of the decision - critic\n",
        "\n",
        "            #action goes from random --> probability distribution\n",
        "            action = np.random.choice(len(action_probs), p=action_probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            #Store experience\n",
        "            states.append(state.flatten())\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            values.append(value)\n",
        "            old_probs.append(action_probs[action]) #to compare with previous policy\n",
        "\n",
        "            # Render environment at specified intervals\n",
        "            if episode % render_interval == 0:\n",
        "                env.render()  # Show the current state of the environment\n",
        "                time.sleep(0.5)  # Add a small delay to visualize the environment\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Train the agent\n",
        "        agent.train(\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(values),\n",
        "            np.array(old_probs),\n",
        "        )\n",
        "        #append to our reward history\n",
        "        reward_history.append(sum(rewards))\n",
        "\n",
        "        #keep track of average reward over time\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            print(\n",
        "                f\"Episode {episode + 1}: Average Reward: {np.mean(reward_history[-50:])}\"\n",
        "            )\n",
        "    #Return our reward history\n",
        "    return reward_history\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    #Initialize enviroment\n",
        "    env = WarehouseEnv()\n",
        "    state_shape = env.state.flatten().shape\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    #Initialize agent\n",
        "    agent = PPOAgent(state_shape, n_actions)\n",
        "    #Train agent\n",
        "    reward_history = train_agent(env, agent)\n",
        "\n",
        "    #Visualize the results\n",
        "    plt.plot(reward_history)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"PPO Training Performance\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgsUS2RUloaf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "class",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}